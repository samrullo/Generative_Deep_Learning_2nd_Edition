{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import (layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first row, first column 11 channel values : [ 0.41925356 -0.5413031   0.58713526  0.6599484   1.6821845  -2.568531\n",
      " -0.7737217   0.5822169   0.4465645  -1.505228   -0.6275001 ]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((2,5,5,11))\n",
    "print(f\"first row, first column 11 channel values : {x[0,0,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_x shape : [2 5 5 3]\n"
     ]
    }
   ],
   "source": [
    "conv_layer =  layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")\n",
    "new_x =conv_layer(x)\n",
    "print(f\"new_x shape : {tf.shape(new_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_x one value : [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"new_x one value : {new_x[0,0,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 : [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x2=conv_layer(x)\n",
    "print(f\"x2 : {x2[0,0,0,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 : [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x3 = conv_layer(x)\n",
    "print(f\"x3 : {x3[0,0,0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor([0.0000, 0.4605, 0.9210, 1.3816, 1.8421, 2.3026, 2.7631, 3.2236, 3.6841,\n",
      "        4.1447, 4.6052, 5.0657, 5.5262, 5.9867, 6.4472, 6.9078])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.log(torch.exp(torch.tensor(2.0))))\n",
    "print(torch.linspace(torch.log(torch.tensor(1.0)), torch.log(torch.tensor(1000.0)),16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    \"\"\"\n",
    "    param x: (N,1) shape random normal noise\n",
    "    \"\"\"\n",
    "    frequencies = torch.linspace(torch.log(torch.tensor(1.0)),torch.log(torch.tensor(1000.0)),16)\n",
    "    frequencies = frequencies.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    angular_speeds = 2*torch.pi*torch.exp(frequencies)*x\n",
    "    return torch.cat((torch.sin(angular_speeds),torch.cos(angular_speeds)),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m noise\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m sin_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msinusoidal_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[109], line 7\u001b[0m, in \u001b[0;36msinusoidal_embedding\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m)),torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1000.0\u001b[39m)),\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      6\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m angular_speeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrequencies\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39msin(angular_speeds),torch\u001b[38;5;241m.\u001b[39mcos(angular_speeds)),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "noise=torch.randn((5,3,32,32))\n",
    "sin_embeddings = sinusoidal_embedding(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 1, 1, 5, 1])\n"
     ]
    }
   ],
   "source": [
    "embeddings = sin_embeddings.unsqueeze(2).unsqueeze(3)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m upsampled_noise \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amrul\\pyvirtualenvs\\dl_311_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amrul\\pyvirtualenvs\\dl_311_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amrul\\pyvirtualenvs\\dl_311_venv\\Lib\\site-packages\\torch\\nn\\modules\\upsampling.py:156\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amrul\\pyvirtualenvs\\dl_311_venv\\Lib\\site-packages\\torch\\nn\\functional.py:4043\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4043\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   4044\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4045\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4046\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4047\u001b[0m )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact (got nearest)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "upsampled_noise = nn.Upsample(scale_factor=64,mode=\"nearest\")(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_noise[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_image = torch.randn(5,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3, padding=1, stride=1)(noisy_image)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([upsampled_noise,x],dim=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1, stride=1)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.AvgPool2d(kernel_size=2)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def ResidualBlock(n_channels):\n",
    "    def apply(x):\n",
    "        in_channels = x.shape[1]\n",
    "        if n_channels==in_channels:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1)(x)\n",
    "        x=nn.BatchNorm2d(num_features=in_channels,affine=False)(x)\n",
    "        x = nn.Conv2d(in_channels=in_channels,out_channels=n_channels, kernel_size=3, padding=1, stride=1)(x)\n",
    "        x = Swish()(x)\n",
    "        x = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, stride=1)(x)\n",
    "        return x + residual\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ResidualBlock(64)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownBlock(n_channels,block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(n_channels)(x)\n",
    "            skips.append(x)\n",
    "        x = nn.AvgPool2d(kernel_size=2)(x)\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips=[]\n",
    "x = DownBlock(96,2)([x,skips])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpBlock(n_channels, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = nn.Upsample(scale_factor=2,mode=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = torch.cat([x,skips.pop()],dim=1)\n",
    "            x = ResidualBlock(n_channels)(x)\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = UpBlock(96,2)([x,skips])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=64, mode=\"nearest\")\n",
    "\n",
    "        self.skips=[]\n",
    "\n",
    "        self.downblock1 = DownBlock(32,2)\n",
    "        self.downblock2 = DownBlock(64,2)\n",
    "        self.downblock3 = DownBlock(96,2)\n",
    "\n",
    "        self.residual1 = ResidualBlock(128)\n",
    "        self.residual2 = ResidualBlock(128)\n",
    "\n",
    "        self.upblock1 = UpBlock(96,2)\n",
    "        self.upblock2 = UpBlock(64, 2)\n",
    "        self.upblock3 = UpBlock(32,2)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        noisy_images,noise_variances = x\n",
    "        noise_embeddings = sinusoidal_embedding(noise_variances)\n",
    "        noise_embeddings = noise_embeddings.unsqueeze(2).unsqueeze(3)\n",
    "        noise_embeddings = self.upsample1(noise_embeddings)\n",
    "        \n",
    "        x = self.conv1(noisy_images)\n",
    "        x = torch.cat([x, noise_embeddings], dim=1)\n",
    "\n",
    "        x = self.downblock1([x, self.skips])\n",
    "        x = self.downblock2([x, self.skips])\n",
    "        x = self.downblock3([x, self.skips])\n",
    "\n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "\n",
    "        x = self.upblock1([x, self.skips])\n",
    "        x = self.upblock2([x, self.skips])\n",
    "        x = self.upblock3([x, self.skips])\n",
    "\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=5\n",
    "noise_variances = torch.randn(N,1)\n",
    "noisy_images = torch.randn(N, 3, 64, 64)\n",
    "\n",
    "unet = Unet(in_channels=3)\n",
    "\n",
    "out = unet([noisy_image, noise_variances])\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate some input data (batch_size, num_channels, height, width)\n",
    "N, C, H, W = 5, 3, 64, 64\n",
    "dummy_train = torch.randn((N, C, H, W))\n",
    "\n",
    "# Compute mean and standard deviation across the channel dimension\n",
    "mean = dummy_train.mean(dim=[0, 2, 3])\n",
    "std = dummy_train.std(dim=[0, 2, 3])\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the normalization transform\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Simulate some input data (batch_size, num_channels, height, width)\n",
    "input_data = torch.randn(16, 3, 32, 32)\n",
    "\n",
    "# Apply the normalization\n",
    "# Note: The input data should be of type torch.FloatTensor and in the range [0, 1] if it's an image\n",
    "normalized_data = normalize(input_data)\n",
    "\n",
    "print(normalized_data.shape)  # Should be (16, 3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean=None\n",
    "        self.std=None\n",
    "    \n",
    "    def adapt(self,train):\n",
    "        self.mean=train.mean(dim=[0,2,3])\n",
    "        self.std=train.std(dim=[0,2,3])\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        normalize_transform = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        return normalize_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.adapt(input_data)\n",
    "out = normalizer.normalize(input_data)\n",
    "print(normalizer.mean, normalizer.std)\n",
    "print(out.shape, out.mean(dim=[0,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_cosine_diffusion_schedule(diffusion_times):\n",
    "    min_rate = 0.02\n",
    "    max_rate = 0.95\n",
    "    start_angle = torch.arccos(torch.tensor(max_rate))\n",
    "    end_angle = torch.arccos(torch.tensor(min_rate))\n",
    "    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "    signal_rates = torch.cos(diffusion_angles)\n",
    "    noise_rates = torch.sin(diffusion_angles)\n",
    "    return noise_rates, signal_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_times = torch.rand((5,1))\n",
    "noise_rates,signal_rates = offset_cosine_diffusion_schedule(diffusion_times)\n",
    "print(noise_rates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, in_channels, adapted_normalizer:Normalizer, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.unet = Unet(in_channels)\n",
    "        self.ema_unet = Unet(in_channels)\n",
    "        self.normalizer = adapted_normalizer\n",
    "    \n",
    "    \n",
    "    def forward(self,images):\n",
    "        images = self.normalizer.normalize(images)\n",
    "        batch_size, n_channels, height, width = images.size()\n",
    "        noises = torch.randn((batch_size, n_channels, height, width))\n",
    "        diffusion_times = torch.rand(batch_size,1)\n",
    "        noise_rates, signal_rates = offset_cosine_diffusion_schedule(diffusion_times)\n",
    "        noise_rates = noise_rates.unsqueeze(2).unsqueeze(3)\n",
    "        signal_rates = signal_rates.unsqueeze(2).unsqueeze(3)\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "        if self.training:\n",
    "            pred_noises = self.unet([noisy_images, noises**2])\n",
    "        else:\n",
    "            pred_noises = self.ema_unet([noisy_images, noises**2])\n",
    "        return pred_noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModel(3,normalizer)\n",
    "out = model(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
