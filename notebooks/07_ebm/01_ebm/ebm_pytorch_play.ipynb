{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"C:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\Generative_Deep_Learning_2nd_Edition\\notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "IMAGE_SIZE=32\n",
    "CHANNELS=1\n",
    "STEP_SIZE=10\n",
    "STEPS=60\n",
    "NOISE=0.005\n",
    "ALPHA=0.1\n",
    "GRADIENT_CLIP=0.03\n",
    "BATCH_SIZE=128\n",
    "BUFFER_SIZE=8192\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "datapath=pathlib.Path(r\"C:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\Generative_Deep_Learning_2nd_Edition\\data\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Pad(2),transforms.ToTensor(),transforms.Normalize(mean=[0.5],std=[0.5])])\n",
    "\n",
    "train_mnist = datasets.MNIST(str(datapath),train=True,download=True,transform=transform)\n",
    "test_mnist = datasets.MNIST(str(datapath),train=False,download=True, transform=transform)\n",
    "\n",
    "print(f\"train mnist size : {len(train_mnist)}, test mnist size : {len(test_mnist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.choice(range(len(train_mnist)))\n",
    "image,label = train_mnist[idx]\n",
    "print(f\"picked index : {idx}\")\n",
    "print(image.shape,label)\n",
    "print(f\"image min : {image.min()}, image max : {image.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(image.flatten().numpy())\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_height(height,kernel_size,stride,padding):\n",
    "    return (height+2*padding-kernel_size)/stride+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"kernel 5, stride 2, padding 1 : {calc_out_height(32,5,2,1)}\")\n",
    "print(f\"height 15, kernel 3, stride 2, padding 1 : {calc_out_height(15,3,2,1)}\")\n",
    "print(f\"height 8, kernel 3, stride 2, padding 1 : {calc_out_height(8,3,2,1)}\")\n",
    "print(f\"height 4, kernel 3, stride 2, padding 1 : {calc_out_height(4,3,2,1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x,beta=1.0):\n",
    "    return x * F.sigmoid(beta*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=swish(image.flatten().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.linspace(-1,-0.9,steps=100)\n",
    "y=swish(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.normal(0,1,(100,1,32,32))\n",
    "flat_images = images.flatten(start_dim=1)\n",
    "print(f\"images shape : {images.shape}, flat_images shape : {flat_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyFunction(nn.Module):\n",
    "    def __init__(self,out_size=2, out_channels=64) -> None:\n",
    "        super(EnergyFunction,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv2=nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.dense = nn.Linear(out_size*out_size*out_channels,64)\n",
    "        self.dense2 = nn.Linear(64,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = swish(self.conv1(x))\n",
    "        x = swish(self.conv2(x))\n",
    "        x = swish(self.conv3(x))\n",
    "        x = swish(self.conv4(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = swish(self.dense(x))\n",
    "        return self.dense2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_mnist,batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.choice(range(len(train_loader)))\n",
    "print(f\"picked idx {idx} from {len(train_loader)} batches\")\n",
    "for idx,(images,labels) in enumerate(train_loader):\n",
    "    if idx == idx:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnergyFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(images)\n",
    "print(f\"out shape : {out.shape}\")\n",
    "print(f\"its mean is : {torch.mean(out,dim=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1 = torch.rand(64,1,32,32)\n",
    "images2 = torch.rand(64,1,32,32)\n",
    "images = torch.cat([images1,images2],dim=0)\n",
    "print(f\"images shape : {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(images)\n",
    "out1,out2 = torch.split(out,out.size(0)//2)\n",
    "print(f\"out1 shape : {out1.shape}, out2 shape : {out2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_CLIP = 0.03\n",
    "import ipdb\n",
    "\n",
    "def generate_samples(model, inp_images, steps, step_size, noise, return_imgs_per_step=False):\n",
    "    imgs_per_step = []     \n",
    "\n",
    "    for _ in range(steps):\n",
    "        inp_images = inp_images.detach()\n",
    "        inp_images.requires_grad_(True)\n",
    "        noised_inp_images = inp_images + torch.normal(0, noise, size = inp_images.size())\n",
    "        noised_inp_images = torch.clamp(noised_inp_images, -1.0, 1.0)               \n",
    "\n",
    "        model.zero_grad()\n",
    "        outscore = model(noised_inp_images)\n",
    "        mean_outscore = torch.mean(outscore,dim=0)\n",
    "        mean_outscore.backward()\n",
    "        #ipdb.set_trace()\n",
    "        grads = torch.clamp(inp_images.grad,-1*GRAD_CLIP,GRAD_CLIP)\n",
    "        #ipdb.set_trace()\n",
    "        inp_images = inp_images + step_size * grads\n",
    "        inp_images = torch.clamp(inp_images, -1.0, 1.0)\n",
    "\n",
    "        if return_imgs_per_step:\n",
    "            imgs_per_step.append(inp_images)\n",
    "    if return_imgs_per_step:\n",
    "        return torch.stack(imgs_per_step,dim=0)\n",
    "    return inp_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatgpt version of generate_samples\n",
    "import torch\n",
    "\n",
    "# Function to generate samples using Langevin Dynamics in PyTorch\n",
    "import torch\n",
    "\n",
    "def generate_samples(\n",
    "    model, inp_imgs, steps, step_size, noise, return_img_per_step=False, return_energy_scores_per_step=False,\n",
    "    gradient_clip=None\n",
    "):\n",
    "    imgs_per_step = []\n",
    "    energy_scores_per_step = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # Ensure inp_imgs is a float tensor and requires grad\n",
    "        inp_imgs = inp_imgs.float().requires_grad_(True)\n",
    "\n",
    "        # Add noise and clip\n",
    "        inp_imgs = inp_imgs + torch.randn_like(inp_imgs) * noise\n",
    "        inp_imgs = torch.clamp(inp_imgs, min=-1.0, max=1.0)\n",
    "\n",
    "        out_score = model(inp_imgs)\n",
    "\n",
    "        # Zero gradients of the model and inp_imgs\n",
    "        model.zero_grad()\n",
    "        if inp_imgs.grad is not None:\n",
    "            inp_imgs.grad.data.zero_()\n",
    "\n",
    "        # Backward pass to get gradients\n",
    "        out_score.sum().backward()\n",
    "\n",
    "        if inp_imgs.grad is None:\n",
    "            raise ValueError(\"No gradients were computed for the input. Check the model's forward pass.\")\n",
    "\n",
    "        grads = inp_imgs.grad.data\n",
    "\n",
    "        # Clipping gradients if a gradient clip value is provided\n",
    "        if gradient_clip is not None:\n",
    "            grads = grads.clamp(min=-gradient_clip, max=gradient_clip)\n",
    "\n",
    "        # Detach inp_imgs from the current graph and update\n",
    "        inp_imgs = inp_imgs.detach() + step_size * grads\n",
    "        inp_imgs = torch.clamp(inp_imgs, min=-1.0, max=1.0)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs.clone().detach())\n",
    "\n",
    "        if return_energy_scores_per_step:\n",
    "            energy_scores_per_step.append(out_score.clone().detach())\n",
    "\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step), torch.stack(energy_scores_per_step)\n",
    "    else:\n",
    "        return inp_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=60\n",
    "step_size = 10\n",
    "noise = 0.005\n",
    "\n",
    "out_images = generate_samples(model,images,steps,step_size,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self,model) -> None:\n",
    "        self.model=model\n",
    "        self.examples = [torch.rand(1,CHANNELS,IMAGE_SIZE,IMAGE_SIZE) for _ in range(BATCH_SIZE)]\n",
    "    \n",
    "    def sample_new_examples(self, steps, step_size, noise):\n",
    "        # number of successes out of running an experiment with binomial outcomes BATCH_SIZE times with 5% probability of success rate\n",
    "        n_new = np.random.binomial(BATCH_SIZE,0.05)\n",
    "        \n",
    "        # we are making values fall in -1.0 to 1.0 range\n",
    "        rand_images = torch.rand(n_new, CHANNELS, IMAGE_SIZE, IMAGE_SIZE) * 2 -1 \n",
    "        \n",
    "        # we are randomly choosing k examples and then concatenating them along batch dimension\n",
    "        old_images = torch.cat(random.choices(self.examples, k=BATCH_SIZE-n_new), dim=0)\n",
    "\n",
    "        # concatenate newly randomly generated samples with picked up existing samples along batch dimension\n",
    "        inp_images = torch.cat([old_images,rand_images], dim=0)\n",
    "\n",
    "        # pass inp_images through langevin dynamics\n",
    "        inp_images = generate_samples(self.model, inp_images, steps, step_size, noise)\n",
    "\n",
    "        # append inp_images to the front of examples\n",
    "        # pay attention to torch.split, in the second argument we are specifying what should be axis 0 size after splitting\n",
    "        # which should equal 1 in this case\n",
    "        self.examples = list(torch.split(inp_images, inp_images.size(0)//BATCH_SIZE, dim=0)) + self.examples\n",
    "\n",
    "        # throw away examples that exceed buffer size\n",
    "        self.examples = self.examples[:BUFFER_SIZE]\n",
    "\n",
    "        return inp_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_images = buffer.sample_new_examples(STEPS,STEP_SIZE,NOISE)\n",
    "print(f\"inp_images shape : {inp_images.shape}, examples size : {len(buffer.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(inp_images.detach().squeeze(1).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
