{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"He goes to school, he studies at school, he has a friend, he has friends. He likes a flower. He likes flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'goes', 'to', 'school', ',', 'he', 'studies', 'at', 'school', ',', 'he', 'has', 'a', 'friend', ',', 'he', 'has', 'friends', '.', 'he', 'likes', 'a', 'flower', '.', 'he', 'likes', 'flowers']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load wine review dataset and build vocabulary from it and build x, y inputs to potentially train our own GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 80\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "import pathlib\n",
    "datasets_folder = pathlib.Path(r\"C:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\Generative_Deep_Learning_2nd_Edition\\data\")\n",
    "wine_review_filepath=datasets_folder/\"wine_reviews\"/\"winemag-data-130k-v2.json\"\n",
    "data = load_dataset(str(wine_review_filepath.parent),'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_with_country_variety(batch):\n",
    "    text_with_country_variety = [f\"{country} : {province} : {variety} : {description}\" for country, province, variety, description in zip(batch['country'],batch['province'], batch['variety'], batch['description'])]\n",
    "    return {\"text\": text_with_country_variety}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = data.map(prepare_text_with_country_variety,batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = wine_ds[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of wine_reviews <class 'list'> and its length 129971\n"
     ]
    }
   ],
   "source": [
    "print(f\"type of wine_reviews {type(wine_reviews)} and its length {len(wine_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generator that yields text from a long list of texts\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a vocabulary from wine_reviews. To achieve this we will use build_vocab_from_iterator of torchtext.vocab module\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# build_vocab_from_iterator accepts a generator that yields text one by one, specials with special tokens when the token falls outside of vocabulary, and finally vocab size to argument max_tokens\n",
    "vocab = build_vocab_from_iterator(yield_tokens(wine_reviews), specials=[\"<unk>\"], max_tokens=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word to index has 10000 elements\n"
     ]
    }
   ],
   "source": [
    "word_to_index = vocab.get_stoi()\n",
    "print(f\"word to index has {len(word_to_index)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's confirm that vocab returns a list of integer IDs just like HuggingFace Tokenizer\n",
    "#tokens = vocab(tokenizer(wine_reviews[0]))\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italy : Sicily & Sardinia : White Blend : Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\n",
      "['italy', 'sicily', '&', 'sardinia', 'white', 'blend', 'aromas', 'include', 'tropical', 'fruit', ',', 'broom', ',', 'brimstone', 'and', 'dried', 'herb', '.', 'the', 'palate', 'isn', \"'\", 't', 'overly', 'expressive', ',', 'offering', 'unripened', 'apple', ',', 'citrus', 'and', 'dried', 'sage', 'alongside', 'brisk', 'acidity', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wine_reviews[0])\n",
    "print(tokenizer(wine_reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(wine_reviews[0])\n",
    "tokens.remove(\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['italy', 'sicily', 'sardinia', 'white', 'blend', 'aromas', 'include', 'tropical', 'fruit', ',', 'broom', ',', 'brimstone', 'and', 'dried', 'herb', '.', 'the', 'palate', 'isn', \"'\", 't', 'overly', 'expressive', ',', 'offering', 'unripened', 'apple', ',', 'citrus', 'and', 'dried', 'sage', 'alongside', 'brisk', 'acidity', '.']\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb=nn.Embedding(10000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([100,200,3000,4000,2345,342,99,10])\n",
    "out=emb(input_ids)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
