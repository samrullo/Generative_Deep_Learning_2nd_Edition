{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 80\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "UNKOWN_WORD = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load wine reviews dataset and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_torch import load_wine_dataset_into_hg_datasets,get_wine_ds_with_country_variety\n",
    "from gpt_torch import get_tokenized_wine_reviews,flatten_tokenized_wine_reviews,get_wine_review_word_to_id,get_wine_review_id_to_word\n",
    "from gpt_torch import tokenize_and_convert_to_ids,batch_tokenize,get_input_ids_as_tensors,get_x_and_y_from_input_ids_tensor,softmax_over_gpt_scores\n",
    "from gpt_torch import TextGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = load_wine_dataset_into_hg_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = get_wine_ds_with_country_variety(wine_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_lower(example):\n",
    "    return {\"text\":example[\"text\"].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = wine_ds.map(text_to_lower,batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"us : new york : pinot gris : fresh apple, lemon and pear flavors are accented by a hint of smoked nuts in this bold, full-bodied pinot gris. rich and a bit creamy in mouthfeel yet balanced briskly, it's a satisfying white with wide pairing appeal. drink now through 2019.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_ds[\"train\"][\"text\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_tokenized is a list of lists. Every element of wr_tokenized is a list of tokens\n",
    "wine_reviews = wine_ds[\"train\"][\"text\"]\n",
    "wr_tokenized = get_tokenized_wine_reviews(wine_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are flattening list of lists. wr_tokens is now a list that consists of individual tokens from the whole dataset\n",
    "wr_tokens = flatten_tokenized_wine_reviews(wr_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are getting mapping of unique words to integer ids\n",
    "wr_word_to_id = get_wine_review_word_to_id(wr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is simply constructing id to word mapping\n",
    "wr_id_to_word = get_wine_review_id_to_word(wr_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fada8da6944e29831ec07ab72cf551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we are tokenizing every sample of wine_ds, which is a review against specific wine\n",
    "# notice we are setting batch_size to None. This will apply batch_tokenize to every individual sample, not a batch of samples. batch_tokenize will receive one review at a time\n",
    "# batch_tokenize will tokenize every review and returns input_ids. It will pad every input_id to a maximum length. This way we can construct a dataset for training our GPT model with consistent shapes\n",
    "wine_ds = wine_ds.map(lambda x : batch_tokenize(x,wr_word_to_id),batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids length : 129971\n"
     ]
    }
   ],
   "source": [
    "input_ids = wine_ds[\"train\"][\"input_ids\"]\n",
    "print(f\"input_ids length : {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us : oregon : cabernet sauvignon : <unk> ' s cab is stylistically apart from either california or washington . it defines its own space . there ' s plenty of new oak , but the fruit , acid and tannins stand up to it . this is sharp and tangy ; cranberry and raspberry , strawberry and citric acids all playing their part . still young , give it some time in a decanter or in your cellar to come\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([wr_id_to_word[id] for id in input_ids[1000]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensors = get_input_ids_as_tensors(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([129971, 81])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_tensors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and y tensors that we will use when training our GPT model\n",
    "x,y = get_x_and_y_from_input_ids_tensor(input_ids_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train, test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_torch import WineReviewDataset\n",
    "\n",
    "wr_torch_dataset = WineReviewDataset(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(wr_torch_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_ds, val_ds, test_ds = random_split(wr_torch_dataset,[train_size,val_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n",
      "batch_x shape : torch.Size([32, 80]), batch_y shape torch.Size([32, 80])\n"
     ]
    }
   ],
   "source": [
    "for idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(f\"batch_x shape : {batch_x.size()}, batch_y shape {batch_y.size()}\")\n",
    "    if idx>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's initialize our precious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_torch import TokenPositionEmbedding,TransformerBlock,GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wr_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "\n",
    "token_pos_embedding = TokenPositionEmbedding(vocab_size,MAX_LEN,EMBEDDING_DIM,device=device)\n",
    "transformer_block = TransformerBlock(N_HEADS,KEY_DIM,EMBEDDING_DIM,FEED_FORWARD_DIM,dropout_rate,device=device)\n",
    "\n",
    "token_pos_embedding = token_pos_embedding.to(device)\n",
    "transformer_block = transformer_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = GPT(token_pos_embedding,transformer_block,EMBEDDING_DIM,vocab_size)\n",
    "gpt_model = gpt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = batch_x.to(device)\n",
    "batch_y = batch_y.to(device)\n",
    "embeddings = token_pos_embedding(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores,attn_weights = gpt_model(batch_x)\n",
    "out=softmax_over_gpt_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this demonstrates that ouput is probability distribution over vocabulary\n",
    "# the sum over probability distribution must equal to 1\n",
    "out[0,0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(wr_word_to_id,wr_id_to_word,gpt_model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe that untrained model generates bunch of gibberish\n",
    "generated_info = text_generator.generate(\"wine review : \",MAX_LEN,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder=pathlib.Path(r\"C:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\Generative_Deep_Learning_2nd_Edition\\notebooks\\09_transformer\\gpt\\checkpoint\")\n",
    "weights_file=\"gpt_pytorch_5.pt\"\n",
    "\n",
    "gpt_model.load_state_dict(torch.load(str(weights_folder/weights_file),map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe how a trained model generates some coherent text\n",
    "generated_info = text_generator.generate(\"italy : \",MAX_LEN, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews[200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
