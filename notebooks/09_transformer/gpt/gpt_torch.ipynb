{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 80\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "UNKOWN_WORD = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load wine reviews dataset and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_torch import load_wine_dataset_into_hg_datasets,get_wine_ds_with_country_variety\n",
    "from gpt_torch import get_tokenized_wine_reviews,flatten_tokenized_wine_reviews,get_wine_review_word_to_id,get_wine_review_id_to_word\n",
    "from gpt_torch import tokenize_and_convert_to_ids,batch_tokenize,get_input_ids_as_tensors,get_x_and_y_from_input_ids_tensor,softmax_over_gpt_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = load_wine_dataset_into_hg_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_ds = get_wine_ds_with_country_variety(wine_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_lower(example):\n",
    "    return {\"text\":example[\"text\"].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting every wine review to lowercase text\n",
    "wine_ds = wine_ds.map(text_to_lower,batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore one sample from wine review\n",
    "wine_ds[\"train\"][\"text\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will produce list of lists of tokens. every wine review is broken down into list of tokens\n",
    "wine_reviews = wine_ds[\"train\"][\"text\"]\n",
    "wr_tokenized = get_tokenized_wine_reviews(wine_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially flattening list of lists\n",
    "wr_tokens = flatten_tokenized_wine_reviews(wr_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building word to id dictionary\n",
    "wr_word_to_id = get_wine_review_word_to_id(wr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and id to word\n",
    "wr_id_to_word = get_wine_review_id_to_word(wr_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing every wine review in the dataset. batch_tokenize will act on every individual wine review. it returns integer representations of tokens as input_ids\n",
    "# it will pad input_ids of every sample to maximum length\n",
    "wine_ds = wine_ds.map(lambda x : batch_tokenize(x,wr_word_to_id),batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = wine_ds[\"train\"][\"input_ids\"]\n",
    "print(f\"input_ids length : {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert every input_id to tensor form\n",
    "input_ids_tensors = get_input_ids_as_tensors(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and y tensors that we will use when training our GPT model\n",
    "# this simply treats the first MAX_LEN elements of input_id as x and shifted values of input_id by one as y\n",
    "x,y = get_x_and_y_from_input_ids_tensor(input_ids_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build torch Dataset and DataLoader for wine reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building pytorch dataset from x and y\n",
    "# remember that pytorch Dataset must implement __len__ and __getitem__\n",
    "from gpt_torch import WineReviewDataset\n",
    "\n",
    "wr_torch_dataset = WineReviewDataset(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train, test and validation datasets\n",
    "total_size = len(wr_torch_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_ds, val_ds, test_ds = random_split(wr_torch_dataset,[train_size,val_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing train, validation and test data loaders which can be used to access data in batches\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just confirm that we can get batches of x and y and they have expected shapes\n",
    "for idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(f\"batch_x shape : {batch_x.size()}, batch_y shape {batch_y.size()}\")\n",
    "    if idx>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's initialize our own precious GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_torch import TokenPositionEmbedding,TransformerBlock,GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wr_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "\n",
    "token_pos_embedding = TokenPositionEmbedding(vocab_size,MAX_LEN,EMBEDDING_DIM,device=device)\n",
    "transformer_block = TransformerBlock(N_HEADS,KEY_DIM,EMBEDDING_DIM,FEED_FORWARD_DIM,dropout_rate,device=device)\n",
    "\n",
    "token_pos_embedding = token_pos_embedding.to(device)\n",
    "transformer_block = transformer_block.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = GPT(token_pos_embedding,transformer_block,EMBEDDING_DIM,vocab_size)\n",
    "gpt_model = gpt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = batch_x.to(device)\n",
    "batch_y = batch_y.to(device)\n",
    "embeddings = token_pos_embedding(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores,attn_weights = gpt_model(batch_x)\n",
    "out=softmax_over_gpt_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0,0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self,word_to_id, id_to_word,gpt_model,device=torch.device(\"cpu\")) -> None:\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.gpt_model = gpt_model\n",
    "        self.device = device\n",
    "    \n",
    "    def sample_from(self,probs,temperature):\n",
    "        probs = probs ** (1/temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs),p=probs), probs\n",
    "    \n",
    "    def generate(self,start_prompt,max_tokens,temperature):\n",
    "        start_tokens = tokenize_and_convert_to_ids(start_prompt,self.word_to_id)\n",
    "        sample_token = None\n",
    "        info = []\n",
    "\n",
    "        while len(start_tokens) < max_tokens and sample_token != self.word_to_id[PAD_TOKEN]:\n",
    "            x = np.array([start_tokens])\n",
    "            x_tensor = torch.tensor(x)\n",
    "            x_tensor = x_tensor.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                scores, attn_weights = self.gpt_model(x_tensor)\n",
    "                y = softmax_over_gpt_scores(scores)\n",
    "                y_np = y.cpu().numpy()\n",
    "                sample_token, probs = self.sample_from(y_np[0][-1],temperature)\n",
    "                info.append({\"prompt\":start_prompt,\"word_probs\":probs,\"attns\":attn_weights[0,-1,:]})\n",
    "                start_tokens.append(sample_token)\n",
    "                start_prompt = f\"{start_prompt} {self.id_to_word[sample_token]}\"\n",
    "        print(f\"generated text : {start_prompt}\")\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(wr_word_to_id,wr_id_to_word,gpt_model,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_info = text_generator.generate(\"wine review : \",MAX_LEN,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train our precious model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model,loss_fn,data_loader,device=torch.device(\"cpu\")):\n",
    "    total_loss = 0\n",
    "    for x,y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gpt_output, attn_weights = model(x)\n",
    "            # remember gpt_output will have shape (N,L,E) while y has (N,L) shape\n",
    "            loss = loss_fn(gpt_output.view(-1, vocab_size),y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model:GPT,text_generator:TextGenerator,loss_fn, train_loader,val_loader,chkpoints_folder:pathlib.Path,max_gen_tokens=MAX_LEN,gen_temperature=1.0,device=torch.device(\"cpu\")):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    generated_infos = []\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in tqdm(range(n_epochs+1),position=0,desc=\"Epoch loop\"):\n",
    "        loss_train = 0\n",
    "        model.train()\n",
    "        for x,y in tqdm(train_loader, position=0, desc=\"Train Loop\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            gpt_output, attn_weights = model(x)\n",
    "\n",
    "            loss = loss_fn(gpt_output.reshape(-1,vocab_size), y.reshape(-1))\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "        loss_train_avg = loss_train/len(train_loader)\n",
    "        print(f\"Epoch {epoch} , average loss : {loss_train_avg}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = compute_loss(model,loss_fn,val_loader,device)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            pt_weights_file = chkpoints_folder/f\"gpt_pytorch_{epoch}.pt\"\n",
    "            torch.save(model.state_dict(),str(pt_weights_file))\n",
    "        val_losses.append(val_loss)\n",
    "        generated_infos.append(text_generator.generate(\"wine review : \",MAX_LEN,gen_temperature))\n",
    "    return train_losses,val_losses,generated_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate=0.001\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gpt_model.parameters(),lr=learning_rate)\n",
    "\n",
    "chkpoints_folder = pathlib.Path(r\"C:\\Users\\amrul\\programming\\deep_learning\\dl_projects\\Generative_Deep_Learning_2nd_Edition\\notebooks\\09_transformer\\gpt\\checkpoint\")\n",
    "\n",
    "train_losses,val_losses,generated_infos = training_loop(EPOCHS,optimizer,gpt_model,text_generator,loss_fn,train_loader,val_loader,chkpoints_folder,MAX_LEN,gen_temperature=0.9,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Train losses\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_losses)\n",
    "plt.title(\"Validation losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=text_generator.generate(\"sweet wine with lemon aftertaste\",MAX_LEN,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_311_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
